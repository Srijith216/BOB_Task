{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "366b2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51852428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fc35111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44963</td>\n",
       "      <td>Boksburg, South Africa</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Best quality couches at unbelievably low price...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44967</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>While we were busy watching election returns a...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                Location     TweetAt  \\\n",
       "0         1       44954             Seattle, WA  02-03-2020   \n",
       "1         2       44956             Chicagoland  02-03-2020   \n",
       "2         3       44959                     NaN  03-03-2020   \n",
       "3         4       44963  Boksburg, South Africa  04-03-2020   \n",
       "4         5       44967          Washington, DC  04-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  When I couldn't find hand sanitizer at Fred Me...  Positive  \n",
       "1  #Panic buying hits #NewYork City as anxious sh...  Negative  \n",
       "2  Voting in the age of #coronavirus = hand sanit...  Positive  \n",
       "3  Best quality couches at unbelievably low price...  Positive  \n",
       "4  While we were busy watching election returns a...  Positive  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets=pd.read_csv('CoronaTweetsSentimentAnalysis.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2b3f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988, 6)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d57367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1988 entries, 0 to 1987\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       1988 non-null   int64 \n",
      " 1   ScreenName     1988 non-null   int64 \n",
      " 2   Location       1536 non-null   object\n",
      " 3   TweetAt        1988 non-null   object\n",
      " 4   OriginalTweet  1988 non-null   object\n",
      " 5   Sentiment      1988 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 93.3+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39e4387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName         False\n",
       "ScreenName       False\n",
       "Location          True\n",
       "TweetAt          False\n",
       "OriginalTweet    False\n",
       "Sentiment        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45df716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Best quality couches at unbelievably low price...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>While we were busy watching election returns a...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1984</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Just been through K?piti New World which is bu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>1985</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Even though the Law Library is closed, ALL sub...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>1986</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@RicePolitics @MDCounties Craig, will you call...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>1987</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1988</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1988 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserName     TweetAt                                      OriginalTweet  \\\n",
       "0            1  02-03-2020  When I couldn't find hand sanitizer at Fred Me...   \n",
       "1            2  02-03-2020  #Panic buying hits #NewYork City as anxious sh...   \n",
       "2            3  03-03-2020  Voting in the age of #coronavirus = hand sanit...   \n",
       "3            4  04-03-2020  Best quality couches at unbelievably low price...   \n",
       "4            5  04-03-2020  While we were busy watching election returns a...   \n",
       "...        ...         ...                                                ...   \n",
       "1983      1984  16-03-2020  Just been through K?piti New World which is bu...   \n",
       "1984      1985  16-03-2020  Even though the Law Library is closed, ALL sub...   \n",
       "1985      1986  16-03-2020  @RicePolitics @MDCounties Craig, will you call...   \n",
       "1986      1987  16-03-2020  Meanwhile In A Supermarket in Israel -- People...   \n",
       "1987      1988  16-03-2020  Did you panic buy a lot of non-perishable item...   \n",
       "\n",
       "     Sentiment  \n",
       "0     Positive  \n",
       "1     Negative  \n",
       "2     Positive  \n",
       "3     Positive  \n",
       "4     Positive  \n",
       "...        ...  \n",
       "1983  Positive  \n",
       "1984  Positive  \n",
       "1985  Negative  \n",
       "1986  Positive  \n",
       "1987  Negative  \n",
       "\n",
       "[1988 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.drop(columns=['Location', 'ScreenName'],inplace = True)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6b5692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = tweets['Sentiment']\n",
    "sns.countplot(data = tweets,x = \"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb79235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\n",
      "#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/IASiReGPC4\r",
      "\r\n",
      "\r",
      "\r\n",
      "#QAnon #QAnon2018 #QAnon2020 \r",
      "\r\n",
      "#Election2020 #CDC https://t.co/29isZOewxu\n"
     ]
    }
   ],
   "source": [
    "print(tweets['OriginalTweet'][0])\n",
    "print(tweets['OriginalTweet'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b74548b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = ' '.join(tweets[tweets['Sentiment'] == 4]['OriginalTweet'].str.lower())\n",
    "negative_tweets = ' '.join(tweets[tweets['Sentiment'] == 0]['OriginalTweet'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "262c84ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n"
     ]
    }
   ],
   "source": [
    "text=tweets['OriginalTweet'].values.tolist()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "507441db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = tweets['OriginalTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "903cb6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SRIJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a08a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,rev in enumerate(reviews.values):\n",
    "    reviews.values[index] = re.sub(r'[^a-zA-Z]', r' ',rev)\n",
    "\n",
    "for index,rev in enumerate(reviews.values):\n",
    "    reviews.values[index] = re.sub(r'br', r' ',rev)\n",
    "    \n",
    "for index,rev in enumerate(reviews.values):\n",
    "    reviews.values[index] = re.sub(r' +', r' ',rev)\n",
    "    \n",
    "for index,rev in enumerate(reviews.values):\n",
    "    reviews.values[index] = rev.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b18c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Cleaned Review'] = reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3973f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8895959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SRIJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SRIJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SRIJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download([\"wordnet\",\"punkt\",\"stopwords\"])\n",
    "es_stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "998b64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,temp in enumerate(reviews.values):\n",
    "    tokens = word_tokenize(temp) # Tokenizing\n",
    "    filtered = [word for word in tokens if not word in es_stop_words]# Removing Stop Words\n",
    "    \n",
    "    temp = \" \".join(filtered)\n",
    "    temp = snow_stemmer.stem(temp)\n",
    "    reviews.values[index] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f09acd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Stemmed Review'] = reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ff2db39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Cleaned Review</th>\n",
       "      <th>Stemmed Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>find hand sanitizer fred meyer turned amazon p...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>when i couldn t find hand sanitizer at fred me...</td>\n",
       "      <td>find hand sanitizer fred meyer turned amazon p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>panic buying hits newyork city anxious shopper...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buying hits newyork city as anxious sho...</td>\n",
       "      <td>panic buying hits newyork city anxious shopper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>voting age coronavirus hand sanitizer supertue...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>voting in the age of coronavirus hand sanitize...</td>\n",
       "      <td>voting age coronavirus hand sanitizer supertue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>best quality couches unbelievably low prices a...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best quality couches at unbelievably low price...</td>\n",
       "      <td>best quality couches unbelievably low prices a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>busy watching election returns acing covid eak...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>while we were busy watching election returns a...</td>\n",
       "      <td>busy watching election returns acing covid eak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName     TweetAt                                      OriginalTweet  \\\n",
       "0         1  02-03-2020  find hand sanitizer fred meyer turned amazon p...   \n",
       "1         2  02-03-2020  panic buying hits newyork city anxious shopper...   \n",
       "2         3  03-03-2020  voting age coronavirus hand sanitizer supertue...   \n",
       "3         4  04-03-2020  best quality couches unbelievably low prices a...   \n",
       "4         5  04-03-2020  busy watching election returns acing covid eak...   \n",
       "\n",
       "  Sentiment                                     Cleaned Review  \\\n",
       "0  Positive  when i couldn t find hand sanitizer at fred me...   \n",
       "1  Negative   panic buying hits newyork city as anxious sho...   \n",
       "2  Positive  voting in the age of coronavirus hand sanitize...   \n",
       "3  Positive  best quality couches at unbelievably low price...   \n",
       "4  Positive  while we were busy watching election returns a...   \n",
       "\n",
       "                                      Stemmed Review  \n",
       "0  find hand sanitizer fred meyer turned amazon p...  \n",
       "1  panic buying hits newyork city anxious shopper...  \n",
       "2  voting age coronavirus hand sanitizer supertue...  \n",
       "3  best quality couches unbelievably low prices a...  \n",
       "4  busy watching election returns acing covid eak...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d025f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SRIJITH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stemming the word\n",
    "nltk.download('wordnet')\n",
    "pt=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "for index,text_ in enumerate(text):\n",
    "    text_=\" \".join(pt.stem(i) for i in text_.split())\n",
    "    text_=\" \".join(wordnet.lemmatize(i) for i in text_.split())  \n",
    "    text[index]=text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2ebfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6375fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for clean_review in tweets['Stemmed Review'].values:\n",
    "    tmp = []\n",
    "    tokens = word_tokenize(clean_review)\n",
    "    filtered = [w.strip() for w in tokens if len(w) > 1]\n",
    "    tmp.extend(filtered)\n",
    "    X.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35ba37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a8e2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd9912e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca70768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(sentences=X,vector_size = 100,window=5,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "468331d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9183"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "780da397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.948005199432373),\n",
       " ('tweet', 0.9403423070907593),\n",
       " ('fb', 0.9342359900474548),\n",
       " ('instagram', 0.9104824066162109),\n",
       " ('chat', 0.8964963555335999),\n",
       " ('hashtag', 0.8885936737060547),\n",
       " ('tweets', 0.8878158330917358),\n",
       " ('tl', 0.8778461217880249),\n",
       " ('link', 0.877821147441864),\n",
       " ('internet', 0.8753897547721863)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-trained model\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "glove_vectors.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d778a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> covid\n",
      "2 --> co\n",
      "3 --> https\n",
      "4 --> coronavirus\n",
      "5 --> food\n",
      "6 --> store\n",
      "7 --> grocery\n",
      "8 --> stock\n",
      "9 --> people\n",
      "10 --> supermarket\n",
      "11 --> shopping\n",
      "12 --> amp\n",
      "13 --> online\n",
      "14 --> panic\n",
      "15 --> prices\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "indexes = tokenizer.word_index\n",
    "for word,num in indexes.items():\n",
    "    print(f\"{num} --> {word}\")\n",
    "    if num == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "30f4dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:42\n",
      "Min length:2\n",
      "Avg length:20.70221327967807\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "min_len = 99999\n",
    "s = 0\n",
    "for l in X:\n",
    "    if max_len < len(l):\n",
    "        max_len = len(l)\n",
    "    if min_len > len(l):\n",
    "        min_len = len(l)\n",
    "    s += len(l)\n",
    "print(f\"Max length:{max_len}\")\n",
    "print(f\"Min length:{min_len}\")\n",
    "print(f\"Avg length:{s/len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ce1ebe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_len = 150\n",
    "X = pad_sequences(X,maxlen=pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34498698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,   14,   23, 1114, 1645,  268,\n",
       "       2173,  275,    8,    5,   12,  423,   41,  276,  823, 1646, 3370,\n",
       "        452,  424,    4, 1329, 3371, 3372,  824,    3,    2, 3373, 1647,\n",
       "       1647, 1647, 1330,  734,    3,    2, 3374])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3d061764",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "41c80865",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = np.zeros((vocab_size,100))\n",
    "#for X,i in indexes.items():\n",
    "    #weight_matrix[i] = w2v_model.wv[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "11a606ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tweets):\n",
    "    # Remove package name as it's not relevant\n",
    "    tweets = tweets.drop('TweetAt', axis=1)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    tweets['Stemmed Review'] = tweets['Stemmed Review'].str.strip().str.lower()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ad1206a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preprocess_data(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6bbbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing data\n",
    "x = tweets['Stemmed Review']\n",
    "y = tweets['Sentiment']\n",
    "x, x_test, y, y_test = train_test_split(x,y, stratify=y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e827cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb3291f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "x = vec.fit_transform(x).toarray()\n",
    "x_test = vec.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1191086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1c3284b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6156941649899397"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0949ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting....\n",
    "model.predict(vec.transform(['Love this app simply awesome!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c25df406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative'], dtype='<U8')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting....\n",
    "model.predict(vec.transform(['I am not satisfied with this!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70237f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e7b3df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b62668f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6f2fb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the negative, positive\n",
    "def sentiment_vader(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return negative, neutral, positive, compound, overall_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ed6ea06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 1.0, 0.7156, 'Positive')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vader(\"wow!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aa8f6beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.33, 0.67, 0.7269, 'Positive')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vader(\"Pretty good, keep it up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b4f67ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3, 0.0, -0.3252, 'Negative')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vader(\"not satisfied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3aa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "15d4261b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c705ce16f248ed9fd538d2f5837cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a49a0f7e719464cabe59c05a42a2e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c56eccc300a49389a23107850322c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ded4c78734ca2bde56c1806dfb117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "04b0a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.19.0.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting pandas-stubs>=1.1.0.11\n",
      "  Downloading pandas_stubs-1.2.0.58-py3-none-any.whl (162 kB)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.59.0)\n",
      "Requirement already satisfied: pandas>=1.2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.2.4)\n",
      "Requirement already satisfied: openpyxl>=3.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (3.0.7)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.25.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\programdata\\anaconda3\\lib\\site-packages (from openpyxl>=3.0.7->openai) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.3->openai) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.3->openai) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.3->openai) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.4)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (PEP 517): started\n",
      "  Building wheel for openai (PEP 517): finished with status 'done'\n",
      "  Created wheel for openai: filename=openai-0.19.0-py3-none-any.whl size=53511 sha256=96f46866cb2e5112da6c96bf8e6f1853b2881d891ccfd85052ea3f7f02e5ca5c\n",
      "  Stored in directory: c:\\users\\srijith\\appdata\\local\\pip\\cache\\wheels\\a0\\1c\\c6\\e258dd9f51667f810902504b9bdeeeb4f2298f72effc65bc2c\n",
      "Successfully built openai\n",
      "Installing collected packages: pandas-stubs, openai\n",
      "Successfully installed openai-0.19.0 pandas-stubs-1.2.0.58\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "123d8ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44963</td>\n",
       "      <td>Boksburg, South Africa</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>Best quality couches at unbelievably low price...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44967</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>04-03-2020</td>\n",
       "      <td>While we were busy watching election returns a...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                Location     TweetAt  \\\n",
       "0         1       44954             Seattle, WA  02-03-2020   \n",
       "1         2       44956             Chicagoland  02-03-2020   \n",
       "2         3       44959                     NaN  03-03-2020   \n",
       "3         4       44963  Boksburg, South Africa  04-03-2020   \n",
       "4         5       44967          Washington, DC  04-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  When I couldn't find hand sanitizer at Fred Me...  Positive  \n",
       "1  #Panic buying hits #NewYork City as anxious sh...  Negative  \n",
       "2  Voting in the age of #coronavirus = hand sanit...  Positive  \n",
       "3  Best quality couches at unbelievably low price...  Positive  \n",
       "4  While we were busy watching election returns a...  Positive  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-3 pre-trained model\n",
    "df = pd.read_csv('CoronaTweetsSentimentAnalysis.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9bb7e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'text': 'prompt', 'sentiment': 'completion'})\n",
    "df.to_csv('openai-parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ce768d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=\"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\"\n",
    "#openai tools fine_tunes.prepare_data -f openai-parsed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4ab8beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,44954,\"Seattle, WA\",02-03-2020,\"When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\",Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('openai-parsed.csv', 'r') as f:\n",
    "    validation_data = list(f)\n",
    "    \n",
    "print(validation_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "388a9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" \\\"This new music video was garbage\\\"\\nSentiment: Negative\\n\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1653802539,\n",
      "  \"id\": \"cmpl-5D5sRpelOjZRlYvIjo6QWqCrLYfz2\",\n",
      "  \"model\": \"davinci:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci:2020-05-03\",\n",
    "    prompt=\"This is a Tweet sentiment classifier\\n\\n\\nSentence: \\\"I loved the new Batman movie!\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"I hate it when my phone battery dies.\\\"\\nSentiment: Negative\\n###\\nSentence: \\\"My day has been 👍\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"This is the link to the article\\\"\\nSentiment: Neutral\\n###\\nSentence: \\\"This new music video blew my mind\\\"\\nSentiment: Positive\\n###\\nSentence:\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=100,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.0,\n",
    "    stop=[\"###\"])\n",
    "print (response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
